{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMajiqLbV6t0nOGV9g0SK9R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miguel72-bit/CORTE2/blob/main/LuisMiguelOrdonezCorte2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes accelerate transformers\n",
        "!pip install gradio peft transformers accelerate bitsandbytes\n",
        "!pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1KCfsM0-aFw",
        "outputId": "6500af03-30b7-440e-c753-bf9c7e706289"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Collecting gradio\n",
            "  Using cached gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Using cached ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Using cached gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Using cached groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Using cached ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Using cached safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Using cached gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
            "Using cached gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "Using cached groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Using cached ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "Using cached safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "Using cached ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Installing collected packages: semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 python-multipart-0.0.20 ruff-0.11.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codigo 1\n"
      ],
      "metadata": {
        "id": "FKTXoMiM7oRS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucaPOYiMsApi",
        "outputId": "fe8c21e1-9d5b-4bea-eedc-56a2841c536a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Usando GPU: Tesla T4\n",
            "Utilizando dispositivo: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Usando GPU: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrada: What color comes out of blue and yellow??\n",
            "Respuesta generada: What color comes out of blue and yellow??\n",
            "\n",
            "I'm not sure if it's a color or a color. I'm not sure if it's a color or a color.\n",
            "\n",
            "I'm not sure if it's a color or a\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import os\n",
        "\n",
        "# Configurar las variables de entorno para la caché de modelos\n",
        "# Establecer la carpeta donde se almacenarán los modelos descargados\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = \"./model_cache\"\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    \"\"\"\n",
        "    Carga un modelo pre-entrenado y su tokenizador correspondiente.\n",
        "\n",
        "    Args:\n",
        "        nombre_modelo (str): Identificador del modelo en Hugging Face Hub\n",
        "\n",
        "    Returns:\n",
        "        tuple: (modelo, tokenizador)\n",
        "    \"\"\"\n",
        "    # Cargar el modelo y el tokenizador desde Hugging Face\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "\n",
        "    # Configurar el modelo para inferencia\n",
        "    modelo = modelo.eval()  # Ponemos el modelo en modo evaluación (sin entrenamiento)\n",
        "\n",
        "    # Si hay GPU disponible, configuramos el modelo para usarla\n",
        "    dispositivo = verificar_dispositivo()\n",
        "    modelo = modelo.to(dispositivo)\n",
        "\n",
        "    return modelo, tokenizador\n",
        "\n",
        "def verificar_dispositivo():\n",
        "    \"\"\"\n",
        "    Verifica el dispositivo disponible (CPU/GPU) y muestra información relevante.\n",
        "\n",
        "    Returns:\n",
        "        torch.device: Dispositivo a utilizar\n",
        "    \"\"\"\n",
        "    # Comprobar si hay GPU disponible\n",
        "    if torch.cuda.is_available():\n",
        "        dispositivo = torch.device(\"cuda\")  # Usar GPU si está disponible\n",
        "        print(f\"✅ Usando GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        dispositivo = torch.device(\"cpu\")  # Si no, usar CPU\n",
        "        print(\"⚠️ Usando CPU (GPU no disponible)\")\n",
        "\n",
        "    return dispositivo\n",
        "\n",
        "# Función principal de prueba\n",
        "def main():\n",
        "    dispositivo = verificar_dispositivo()\n",
        "    print(f\"Utilizando dispositivo: {dispositivo}\")\n",
        "\n",
        "    # Cargar un modelo pequeño adecuado para chatbots, por ejemplo, GPT-2\n",
        "    modelo, tokenizador = cargar_modelo(\"gpt2\")  # Puedes reemplazar \"gpt2\" por otro modelo\n",
        "\n",
        "    # Realizar una prueba simple de generación de texto\n",
        "    entrada = \"What color comes out of blue and yellow??\"\n",
        "    inputs = tokenizador(entrada, return_tensors=\"pt\").to(dispositivo)\n",
        "\n",
        "    # Generar una respuesta usando el modelo\n",
        "    with torch.no_grad():\n",
        "        salida = modelo.generate(inputs[\"input_ids\"], max_length=50, num_return_sequences=1)\n",
        "\n",
        "    # Decodificar la salida generada\n",
        "    respuesta = tokenizador.decode(salida[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"Entrada: {entrada}\")\n",
        "    print(f\"Respuesta generada: {respuesta}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codigo 2"
      ],
      "metadata": {
        "id": "cTp9lLeM7y7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Configuración del modelo y tokenizador\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    \"\"\"\n",
        "    Carga un modelo pre-entrenado y su tokenizador correspondiente.\n",
        "\n",
        "    Args:\n",
        "        nombre_modelo (str): Identificador del modelo en Hugging Face Hub\n",
        "\n",
        "    Returns:\n",
        "        tuple: (modelo, tokenizador)\n",
        "    \"\"\"\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "\n",
        "    # Asignar el token de padding si no tiene\n",
        "    if tokenizador.pad_token is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token  # Usamos el token EOS como PAD\n",
        "\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelo = modelo.to(dispositivo)  # Mover el modelo a la GPU si está disponible\n",
        "\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "def preprocesar_entrada(texto, tokenizador, longitud_maxima=512, dispositivo=None):\n",
        "    \"\"\"\n",
        "    Preprocesa el texto de entrada para pasarlo al modelo.\n",
        "\n",
        "    Args:\n",
        "        texto (str): Texto de entrada del usuario\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        longitud_maxima (int): Longitud máxima de la secuencia\n",
        "        dispositivo (torch.device): Dispositivo donde ejecutar el modelo\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor de entrada para el modelo\n",
        "    \"\"\"\n",
        "    # Añadir tokens especiales y convertir a tensor\n",
        "    tokens = tokenizador.encode(texto, truncation=True, padding=\"max_length\", max_length=longitud_maxima, return_tensors=\"pt\")\n",
        "\n",
        "    # Pasar los tokens al dispositivo adecuado (GPU/CPU)\n",
        "    tokens = tokens.to(dispositivo)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def generar_respuesta(modelo, entrada_procesada, tokenizador, parametros_generacion=None):\n",
        "    \"\"\"\n",
        "    Genera una respuesta basada en la entrada procesada.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo de lenguaje\n",
        "        entrada_procesada: Tokens de entrada procesados\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        parametros_generacion (dict): Parámetros para controlar la generación\n",
        "\n",
        "    Returns:\n",
        "        str: Respuesta generada\n",
        "    \"\"\"\n",
        "    if parametros_generacion is None:\n",
        "        parametros_generacion = {\n",
        "            \"do_sample\": True,  # Activar el muestreo para usar temperature y top_p\n",
        "            \"max_new_tokens\": 50,  # Generar hasta 50 tokens nuevos\n",
        "            \"temperature\": 0.7,  # Controla la aleatoriedad\n",
        "            \"top_p\": 0.9,  # Controla el muestreo (top-p sampling)\n",
        "            \"top_k\": 50,  # Número de palabras candidatas a considerar\n",
        "        }\n",
        "\n",
        "    # Generar la respuesta usando el modelo\n",
        "    with torch.no_grad():\n",
        "        salida = modelo.generate(entrada_procesada, **parametros_generacion)\n",
        "\n",
        "    # Decodificar la salida generada\n",
        "    respuesta = tokenizador.decode(salida[0], skip_special_tokens=True)\n",
        "\n",
        "    return respuesta\n",
        "\n",
        "def crear_prompt_sistema(instrucciones):\n",
        "    \"\"\"\n",
        "    Crea un prompt de sistema para dar instrucciones al modelo.\n",
        "\n",
        "    Args:\n",
        "        instrucciones (str): Instrucciones sobre cómo debe comportarse el chatbot\n",
        "\n",
        "    Returns:\n",
        "        str: Prompt formateado\n",
        "    \"\"\"\n",
        "    return f\"El siguiente es un chatbot. Instrucciones: {instrucciones}\\n\"\n",
        "\n",
        "# Ejemplo de uso\n",
        "def interaccion_simple():\n",
        "    modelo, tokenizador, dispositivo = cargar_modelo(\"gpt2\")\n",
        "\n",
        "    # Crear un prompt de sistema para definir la personalidad del chatbot\n",
        "    instrucciones = \"El chatbot debe ser amistoso, educado y brindar respuestas claras y concisas.\"\n",
        "    prompt_sistema = crear_prompt_sistema(instrucciones)\n",
        "\n",
        "    # Procesa una entrada de ejemplo\n",
        "    entrada_usuario = \"¿What color comes out of red and yellow??\"\n",
        "    entrada_procesada = preprocesar_entrada(prompt_sistema + entrada_usuario, tokenizador, dispositivo=dispositivo)\n",
        "\n",
        "    # Genera y mostrar la respuesta\n",
        "    respuesta = generar_respuesta(modelo, entrada_procesada, tokenizador)\n",
        "    print(f\"Respuesta del chatbot: {respuesta}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interaccion_simple()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcfakA502X48",
        "outputId": "80ae2632-916b-487d-f074-e6689a03c923"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta del chatbot: El siguiente es un chatbot. Instrucciones: El chatbot debe ser amistoso, educado y brindar respuestas claras y concisas.\n",
            "¿What color comes out of red and yellow??The new version of the U.S. Constitution, U.S. Code, is the most comprehensive and comprehensive of the U.S. Constitution. It also provides a blueprint for how to live and work in the United States.\n",
            "\n",
            "It\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codigo 3"
      ],
      "metadata": {
        "id": "qcva6UJo77DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    \"\"\"\n",
        "    Carga un modelo pre-entrenado y su tokenizador correspondiente.\n",
        "\n",
        "    Args:\n",
        "        nombre_modelo (str): Identificador del modelo en Hugging Face Hub\n",
        "\n",
        "    Returns:\n",
        "        tuple: (modelo, tokenizador, dispositivo)\n",
        "    \"\"\"\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "\n",
        "\n",
        "    if tokenizador.pad_token is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token  # Usamos el token EOS como PAD\n",
        "\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelo = modelo.to(dispositivo)  # Mover el modelo a la GPU si está disponible\n",
        "\n",
        "    return modelo, tokenizador, dispositivo  # Devolver tres valores\n",
        "\n",
        "class GestorContexto:\n",
        "    \"\"\"\n",
        "    Clase para gestionar el contexto de una conversación con el chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, longitud_maxima=1024, formato_mensaje=None):\n",
        "        self.historial = []\n",
        "        self.longitud_maxima = longitud_maxima\n",
        "        self.formato_mensaje = formato_mensaje or self._formato_predeterminado\n",
        "\n",
        "    def _formato_predeterminado(self, rol, contenido):\n",
        "        return f\"{rol}: {contenido}\"\n",
        "\n",
        "    def agregar_mensaje(self, rol, contenido):\n",
        "        mensaje_formateado = self.formato_mensaje(rol, contenido)\n",
        "        self.historial.append(mensaje_formateado)\n",
        "\n",
        "    def construir_prompt_completo(self):\n",
        "        return \"\\n\".join(self.historial)\n",
        "\n",
        "    def truncar_historial(self, tokenizador):\n",
        "        while True:\n",
        "            tokens = tokenizador.encode(self.construir_prompt_completo(), return_tensors=\"pt\")\n",
        "            if len(tokens[0]) > self.longitud_maxima:\n",
        "                self.historial.pop(0)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "class Chatbot:\n",
        "    \"\"\"\n",
        "    Implementación de chatbot con manejo de contexto.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, modelo_id, instrucciones_sistema=None):\n",
        "        \"\"\"\n",
        "        Inicializa el chatbot.\n",
        "\n",
        "        Args:\n",
        "            modelo_id (str): Identificador del modelo en Hugging Face\n",
        "            instrucciones_sistema (str): Instrucciones de comportamiento del sistema\n",
        "        \"\"\"\n",
        "        # Desempaquetar los tres valores: modelo, tokenizador, y dispositivo\n",
        "        self.modelo, self.tokenizador, self.dispositivo = cargar_modelo(modelo_id)\n",
        "        self.gestor_contexto = GestorContexto()\n",
        "\n",
        "        # Inicializar el contexto con instrucciones del sistema\n",
        "        if instrucciones_sistema:\n",
        "            self.gestor_contexto.agregar_mensaje('sistema', instrucciones_sistema)\n",
        "\n",
        "    def responder(self, mensaje_usuario, parametros_generacion=None):\n",
        "        \"\"\"\n",
        "        Genera una respuesta al mensaje del usuario.\n",
        "\n",
        "        Args:\n",
        "            mensaje_usuario (str): Mensaje del usuario\n",
        "            parametros_generacion (dict): Parámetros para la generación\n",
        "\n",
        "        Returns:\n",
        "            str: Respuesta generada\n",
        "        \"\"\"\n",
        "        # 1. Agregar mensaje del usuario al contexto\n",
        "        self.gestor_contexto.agregar_mensaje('usuario', mensaje_usuario)\n",
        "\n",
        "        # 2. Construir el prompt completo\n",
        "        prompt_completo = self.gestor_contexto.construir_prompt_completo()\n",
        "\n",
        "        # 3. Preprocesar la entrada\n",
        "        entrada_procesada = preprocesar_entrada(prompt_completo, self.tokenizador, dispositivo=self.dispositivo)\n",
        "\n",
        "        # 4. Generar la respuesta\n",
        "        respuesta = generar_respuesta(self.modelo, entrada_procesada, self.tokenizador, parametros_generacion)\n",
        "\n",
        "        # 5. Agregar respuesta al contexto\n",
        "        self.gestor_contexto.agregar_mensaje('asistente', respuesta)\n",
        "\n",
        "        # 6. Devolver la respuesta\n",
        "        return respuesta\n",
        "\n",
        "# Funciones auxiliares\n",
        "def preprocesar_entrada(texto, tokenizador, longitud_maxima=512, dispositivo=None):\n",
        "    tokens = tokenizador.encode(texto, truncation=True, padding=\"max_length\", max_length=longitud_maxima, return_tensors=\"pt\")\n",
        "    tokens = tokens.to(dispositivo)\n",
        "    return tokens\n",
        "\n",
        "def generar_respuesta(modelo, entrada_procesada, tokenizador, parametros_generacion=None):\n",
        "    if parametros_generacion is None:\n",
        "        parametros_generacion = {\n",
        "            \"do_sample\": True,\n",
        "            \"max_new_tokens\": 50,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"top_k\": 50,\n",
        "        }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        salida = modelo.generate(entrada_procesada, **parametros_generacion)\n",
        "\n",
        "    respuesta = tokenizador.decode(salida[0], skip_special_tokens=True)\n",
        "    return respuesta\n",
        "\n",
        "# Prueba del sistema\n",
        "def prueba_conversacion():\n",
        "    instrucciones = \"El chatbot debe ser amigable, siempre responde de manera educada y concisa.\"\n",
        "    chatbot = Chatbot(modelo_id=\"gpt2\", instrucciones_sistema=instrucciones)\n",
        "\n",
        "    print(chatbot.responder(\"¿que mescla sale de rojo y amarillo?\"))\n",
        "    print(chatbot.responder(\"¿Qué puedes hacer por mí?\"))\n",
        "    print(chatbot.responder(\"¿quien descubrio america?\"))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prueba_conversacion()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49epQCPz3-hI",
        "outputId": "178d7851-1145-4e00-cf49-294684f94a0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sistema: El chatbot debe ser amigable, siempre responde de manera educada y concisa.\n",
            "usuario: ¿que mescla sale de rojo y amarillo?The most recent example of a new invention that could help the development of robots is a new robotic arm that could be used to help with the development of robots.\n",
            "\n",
            "The project is called the \"Robot Robotic Arm\" and has been in\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sistema: El chatbot debe ser amigable, siempre responde de manera educada y concisa.\n",
            "usuario: ¿que mescla sale de rojo y amarillo?\n",
            "asistente: sistema: El chatbot debe ser amigable, siempre responde de manera educada y concisa.\n",
            "usuario: ¿que mescla sale de rojo y amarillo?The most recent example of a new invention that could help the development of robots is a new robotic arm that could be used to help with the development of robots.\n",
            "\n",
            "The project is called the \"Robot Robotic Arm\" and has been in\n",
            "usuario: ¿Qué puedes hacer por mí?There are more than a few things you can do to help your family and friends get over the stresses of a job.\n",
            "\n",
            "A common problem for many workers is that they don't have a family home and don't have a car.\n",
            "\n",
            "\n",
            "sistema: El chatbot debe ser amigable, siempre responde de manera educada y concisa.\n",
            "usuario: ¿que mescla sale de rojo y amarillo?\n",
            "asistente: sistema: El chatbot debe ser amigable, siempre responde de manera educada y concisa.\n",
            "usuario: ¿que mescla sale de rojo y amarillo?The most recent example of a new invention that could help the development of robots is a new robotic arm that could be used to help with the development of robots.\n",
            "\n",
            "The project is called the \"Robot Robotic Arm\" and has been in\n",
            "usuario: ¿Qué puedes hacer por mí?\n",
            "asistente: sistema: El chatbot debe ser amigable, siempre responde de manera educada y concisa.\n",
            "usuario: ¿que mescla sale de rojo y amarillo?\n",
            "asistente: sistema: El chatbot debe ser amigable, siempre responde de manera educada y concisa.\n",
            "usuario: ¿que mescla sale de rojo y amarillo?The most recent example of a new invention that could help the development of robots is a new robotic arm that could be used to help with the development of robots.\n",
            "\n",
            "The project is called the \"Robot Robotic Arm\" and has been in\n",
            "usuario: ¿Qué puedes hacer por mí?There are more than a few things you can do to help your family and friends get over the stresses of a job.\n",
            "\n",
            "A common problem for many workers is that they don't have a family home and don't have a car.\n",
            "\n",
            "\n",
            "usuario: ¿quien descubrio america?What is it?\n",
            "\n",
            "\n",
            "It is a computer that is powered by a computer and runs on a computer that is connected to a computer with an internet connection.\n",
            "\n",
            "The computer is connected to a computer through a network of servers or other devices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codigo 4"
      ],
      "metadata": {
        "id": "sSW4fzxT7_td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import time\n",
        "import torch.nn as nn\n",
        "\n",
        "#  importar bitsandbytes y configurar cuantización\n",
        "try:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    bitsandbytes_available = True\n",
        "    print(\"`bitsandbytes` está instalado.\")\n",
        "except ImportError:\n",
        "    bitsandbytes_available = False\n",
        "    print(\"`bitsandbytes` no está instalado, se omitirá la cuantización.\")\n",
        "\n",
        "def configurar_cuantizacion(bits=4):\n",
        "    \"\"\"\n",
        "    Configura los parámetros para la cuantización del modelo.\n",
        "\n",
        "    Args:\n",
        "        bits (int): Bits para cuantización (4 u 8)\n",
        "\n",
        "    Returns:\n",
        "        BitsAndBytesConfig: Configuración de cuantización\n",
        "    \"\"\"\n",
        "    if bitsandbytes_available:\n",
        "        # Configuración para 4 bits o 8 bits de cuantización\n",
        "        config_cuantizacion = BitsAndBytesConfig(\n",
        "            load_in_4bit=(bits == 4),\n",
        "            load_in_8bit=(bits == 8)\n",
        "        )\n",
        "        return config_cuantizacion\n",
        "    else:\n",
        "        print(\"Cuantización no disponible sin `bitsandbytes`. Se continuará sin cuantización.\")\n",
        "        return None\n",
        "\n",
        "def cargar_modelo_optimizado(nombre_modelo, optimizaciones=None):\n",
        "    \"\"\"\n",
        "    Carga un modelo con optimizaciones aplicadas.\n",
        "\n",
        "    Args:\n",
        "        nombre_modelo (str): Identificador del modelo\n",
        "        optimizaciones (dict): Diccionario con flags para las optimizaciones\n",
        "\n",
        "    Returns:\n",
        "        tuple: (modelo, tokenizador)\n",
        "    \"\"\"\n",
        "    if optimizaciones is None:\n",
        "        optimizaciones = {\n",
        "            \"cuantizacion\": True,\n",
        "            \"bits\": 4,\n",
        "            \"offload_cpu\": False,\n",
        "            \"flash_attention\": True\n",
        "        }\n",
        "\n",
        "    # Aplicar cuantización si está habilitada\n",
        "    if optimizaciones.get(\"cuantizacion\", False) and bitsandbytes_available:\n",
        "        config_cuantizacion = configurar_cuantizacion(optimizaciones[\"bits\"])\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo, quantization_config=config_cuantizacion)\n",
        "    else:\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "\n",
        "\n",
        "    # Aquí solo cargamos el modelo con cuantización por simplicidad\n",
        "\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelo.to(dispositivo)\n",
        "\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "def aplicar_sliding_window(modelo, window_size=1024):\n",
        "    \"\"\"\n",
        "    Configura la atención de ventana deslizante para procesar secuencias largas.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo a configurar\n",
        "        window_size (int): Tamaño de la ventana de atención\n",
        "    \"\"\"\n",
        "    # Aquí aplicamos una función de atención deslizante para que el modelo maneje secuencias largas\n",
        "    # Configurar una ventana deslizante en la capa de atención\n",
        "    for layer in modelo.transformer.h:\n",
        "        layer.attn.config.attn_window_size = window_size\n",
        "    print(f\"Ventana de atención deslizante configurada con tamaño: {window_size}\")\n",
        "\n",
        "def evaluar_rendimiento(modelo, tokenizador, texto_prueba, dispositivo):\n",
        "    \"\"\"\n",
        "    Evalúa el rendimiento del modelo en términos de velocidad y memoria.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo a evaluar\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        texto_prueba (str): Texto para pruebas de rendimiento\n",
        "        dispositivo: Dispositivo donde se ejecutará\n",
        "\n",
        "    Returns:\n",
        "        dict: Métricas de rendimiento\n",
        "    \"\"\"\n",
        "    # Preprocesar la entrada\n",
        "    inputs = tokenizador(texto_prueba, return_tensors=\"pt\").to(dispositivo)\n",
        "\n",
        "    # Medir el tiempo de inferencia\n",
        "    inicio = time.time()\n",
        "    with torch.no_grad():\n",
        "        modelo(inputs[\"input_ids\"])\n",
        "    fin = time.time()\n",
        "\n",
        "    tiempo_inferencia = fin - inicio\n",
        "\n",
        "    # Uso de memoria\n",
        "    memoria_MB = torch.cuda.memory_allocated() / (1024 ** 2) if torch.cuda.is_available() else 0\n",
        "\n",
        "    # Métricas adicionales (tokens por segundo)\n",
        "    tokens_por_segundo = len(inputs[\"input_ids\"]) / tiempo_inferencia if tiempo_inferencia > 0 else 0\n",
        "\n",
        "    # Retornar las métricas\n",
        "    return {\n",
        "        \"tiempo_inferencia\": tiempo_inferencia,\n",
        "        \"memoria_MB\": memoria_MB,\n",
        "        \"tokens_por_segundo\": tokens_por_segundo\n",
        "    }\n",
        "\n",
        "# Función de demostración\n",
        "def demo_optimizaciones():\n",
        "    # Crear y evaluar diferentes configuraciones\n",
        "    modelo_base, tokenizador, dispositivo = cargar_modelo_optimizado(\"gpt2\", optimizaciones={\"cuantizacion\": False})\n",
        "    modelo_4bits, _, _ = cargar_modelo_optimizado(\"gpt2\", optimizaciones={\"cuantizacion\": True, \"bits\": 4})\n",
        "    modelo_sliding_window, _, _ = cargar_modelo_optimizado(\"gpt2\", optimizaciones={\"cuantizacion\": False})\n",
        "\n",
        "    aplicar_sliding_window(modelo_sliding_window)\n",
        "\n",
        "    # Texto de prueba para evaluar\n",
        "    texto_prueba = \"Hola, ¿cómo estás? Esta es una prueba para evaluar el rendimiento del modelo.\"\n",
        "\n",
        "    # Evaluación del rendimiento con el modelo base\n",
        "    print(\"Evaluación del modelo base:\")\n",
        "    metricas_base = evaluar_rendimiento(modelo_base, tokenizador, texto_prueba, dispositivo)\n",
        "    print(metricas_base)\n",
        "\n",
        "    # Evaluación con el modelo con cuantización de 4 bits\n",
        "    print(\"Evaluación con el modelo con cuantización de 4 bits:\")\n",
        "    metricas_4bits = evaluar_rendimiento(modelo_4bits, tokenizador, texto_prueba, dispositivo)\n",
        "    print(metricas_4bits)\n",
        "\n",
        "    # Evaluación con el modelo con ventana deslizante\n",
        "    print(\"Evaluación con el modelo con ventana deslizante:\")\n",
        "    metricas_sliding_window = evaluar_rendimiento(modelo_sliding_window, tokenizador, texto_prueba, dispositivo)\n",
        "    print(metricas_sliding_window)\n",
        "\n",
        "    # Comparar y mostrar las métricas de rendimiento\n",
        "    print(\"Comparación de métricas de rendimiento:\")\n",
        "    print(f\"Modelo base vs 4 bits: {metricas_base['tiempo_inferencia']} segundos vs {metricas_4bits['tiempo_inferencia']} segundos\")\n",
        "    print(f\"Modelo base vs Sliding Window: {metricas_base['tiempo_inferencia']} segundos vs {metricas_sliding_window['tiempo_inferencia']} segundos\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_optimizaciones()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DE9D7xF8E3P",
        "outputId": "83de92e7-540b-4289-c6e8-95df430199be"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`bitsandbytes` está instalado.\n",
            "Ventana de atención deslizante configurada con tamaño: 1024\n",
            "Evaluación del modelo base:\n",
            "{'tiempo_inferencia': 0.012219905853271484, 'memoria_MB': 5805.74560546875, 'tokens_por_segundo': 81.83369102899286}\n",
            "Evaluación con el modelo con cuantización de 4 bits:\n",
            "{'tiempo_inferencia': 0.023812532424926758, 'memoria_MB': 5805.74560546875, 'tokens_por_segundo': 41.994693472971754}\n",
            "Evaluación con el modelo con ventana deslizante:\n",
            "{'tiempo_inferencia': 0.010013580322265625, 'memoria_MB': 5805.74560546875, 'tokens_por_segundo': 99.86438095238096}\n",
            "Comparación de métricas de rendimiento:\n",
            "Modelo base vs 4 bits: 0.012219905853271484 segundos vs 0.023812532424926758 segundos\n",
            "Modelo base vs Sliding Window: 0.012219905853271484 segundos vs 0.010013580322265625 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codigo 5"
      ],
      "metadata": {
        "id": "SahksaK4_yQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio transformers torch\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2CFHpDZA4T1",
        "outputId": "be8e0c46-e8d8-4846-cbc7-fae80a8edf12"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.29.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Clase para manejar el contexto de la conversación\n",
        "class GestorContexto:\n",
        "    def __init__(self, longitud_maxima=1024, formato_mensaje=None):\n",
        "        self.historial = []\n",
        "        self.longitud_maxima = longitud_maxima\n",
        "        self.formato_mensaje = formato_mensaje or self._formato_predeterminado\n",
        "\n",
        "    def _formato_predeterminado(self, rol, contenido):\n",
        "        return f\"{rol}: {contenido}\"\n",
        "\n",
        "    def agregar_mensaje(self, rol, contenido):\n",
        "        mensaje_formateado = self.formato_mensaje(rol, contenido)\n",
        "        self.historial.append(mensaje_formateado)\n",
        "\n",
        "    def construir_prompt_completo(self):\n",
        "        # Solo mantenemos los últimos mensajes que sean relevantes\n",
        "        return \"\\n\".join(self.historial[-5:])  # Tomamos solo los últimos 5 mensajes\n",
        "\n",
        "    def truncar_historial(self, tokenizador):\n",
        "        while True:\n",
        "            tokens = tokenizador.encode(self.construir_prompt_completo(), return_tensors=\"pt\")\n",
        "            if len(tokens[0]) > self.longitud_maxima:\n",
        "                self.historial.pop(0)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "# Función para cargar el modelo optimizado (cuantización opcional)\n",
        "def cargar_modelo_optimizado(nombre_modelo):\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "\n",
        "    # Verificar si el tokenizador tiene un pad_token, si no, usar el eos_token como pad_token\n",
        "    if tokenizador.pad_token is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token  # Usamos el token EOS como PAD\n",
        "\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelo.to(dispositivo)\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "# Preprocesamiento de la entrada\n",
        "def preprocesar_entrada(texto, tokenizador, longitud_maxima=512, dispositivo=None):\n",
        "    tokens = tokenizador.encode(texto, truncation=True, padding=\"max_length\", max_length=longitud_maxima, return_tensors=\"pt\")\n",
        "    tokens = tokens.to(dispositivo)\n",
        "    return tokens\n",
        "\n",
        "# Generación de la respuesta\n",
        "def generar_respuesta(modelo, entrada_procesada, tokenizador, parametros_generacion=None):\n",
        "    if parametros_generacion is None:\n",
        "        parametros_generacion = {\n",
        "            \"do_sample\": True,\n",
        "            \"max_new_tokens\": 50,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"top_k\": 50,\n",
        "        }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        salida = modelo.generate(entrada_procesada, **parametros_generacion)\n",
        "\n",
        "    respuesta = tokenizador.decode(salida[0], skip_special_tokens=True)\n",
        "    return respuesta\n",
        "\n",
        "# Clase del chatbot\n",
        "class Chatbot:\n",
        "    def __init__(self, modelo_id, instrucciones_sistema=None):\n",
        "        self.modelo, self.tokenizador, self.dispositivo = cargar_modelo_optimizado(modelo_id)\n",
        "        self.gestor_contexto = GestorContexto()\n",
        "\n",
        "        if instrucciones_sistema:\n",
        "            self.gestor_contexto.agregar_mensaje('sistema', instrucciones_sistema)\n",
        "\n",
        "    def responder(self, mensaje_usuario):\n",
        "        self.gestor_contexto.agregar_mensaje('usuario', mensaje_usuario)\n",
        "        prompt_completo = self.gestor_contexto.construir_prompt_completo()\n",
        "        entrada_procesada = preprocesar_entrada(prompt_completo, self.tokenizador, dispositivo=self.dispositivo)\n",
        "        respuesta = generar_respuesta(self.modelo, entrada_procesada, self.tokenizador)\n",
        "        self.gestor_contexto.agregar_mensaje('asistente', respuesta)\n",
        "        return respuesta\n",
        "\n",
        "# Función para crear la interfaz web con Gradio\n",
        "def crear_interfaz_web(chatbot):\n",
        "    \"\"\"\n",
        "    Crea una interfaz web simple para el chatbot usando Gradio.\n",
        "\n",
        "    Args:\n",
        "        chatbot: Instancia del chatbot\n",
        "\n",
        "    Returns:\n",
        "        gr.Interface: Interfaz de Gradio\n",
        "    \"\"\"\n",
        "    # Función de callback para procesar entradas y generar respuestas\n",
        "    def procesar_entrada(mensaje_usuario):\n",
        "        return chatbot.responder(mensaje_usuario)\n",
        "\n",
        "    # Definir los componentes de la interfaz\n",
        "    interfaz = gr.Interface(\n",
        "        fn=procesar_entrada,  # Función de procesamiento\n",
        "        inputs=gr.Textbox(lines=2, placeholder=\"Escribe tu mensaje aquí...\"),  # Entrada de texto\n",
        "        outputs=gr.Textbox(),  # Salida de texto\n",
        "        title=\"Chatbot Personalizado\",  # Título de la interfaz\n",
        "        description=\"Chatbot que responde preguntas y mantiene el contexto de la conversación.\"  # Descripción\n",
        "    )\n",
        "\n",
        "    return interfaz\n",
        "\n",
        "# Función principal para el despliegue\n",
        "def main_despliegue():\n",
        "    # Cargar el modelo personalizado\n",
        "    modelo_id = \"gpt2\"  # O usar tu modelo personalizado si lo tienes\n",
        "    instrucciones_sistema = \"El chatbot debe ser amigable, educado y conciso.\"\n",
        "\n",
        "    # Crear instancia del chatbot\n",
        "    chatbot = Chatbot(modelo_id, instrucciones_sistema)\n",
        "\n",
        "    # Crear y lanzar la interfaz web\n",
        "    interfaz = crear_interfaz_web(chatbot)\n",
        "    interfaz.launch(share=True)  # share=True para obtener un enlace público de la interfaz\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_despliegue()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "hVhigEmf_xUe",
        "outputId": "417f8a8e-0b62-4ec7-8cfe-ce8ff7c8fd43"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d8f2b3780c77072fb6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d8f2b3780c77072fb6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}